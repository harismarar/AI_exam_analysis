{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b7f263-baa3-4016-936f-671036470d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\AI_interview\\final\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_frames(video_path, output_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / fps\n",
    "\n",
    "    # Calculate the frame interval to get n/2 frames\n",
    "    frame_interval = int((total_frames * 4) /  duration)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # Loop through the video and extract frames\n",
    "    current_frame = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break  # Break the loop if no more frames are available\n",
    "\n",
    "        # Save the frame if it's at the interval\n",
    "        if current_frame % frame_interval == 0:\n",
    "            frame_filename = f\"{output_path}/frame_{current_frame}.jpg\"\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "\n",
    "        current_frame += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a916ae-74f3-49a6-8226-b507a2d9edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(frame_path):\n",
    "    img = cv2.imread(frame_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale if needed\n",
    "    img = cv2.resize(img, (48, 48))  # Resize to match the input size of the model\n",
    "    img = img / 255.0  # Normalize pixel values\n",
    "    \n",
    "    img = img.reshape((1,) + img.shape + (1,))\n",
    "    \n",
    "    # Make prediction\n",
    "    emotion_probs = emotion_model.predict(img)\n",
    "    \n",
    "    # Get the emotion with the highest probability\n",
    "    predicted_emotion = emotion_classes[np.argmax(emotion_probs)]\n",
    "    \n",
    "    return predicted_emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8215e24c-fef2-44b8-8555-4fc79915f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_emotion_score(emotion):\n",
    "    emotion_scores = {'angry': 40, 'fear': 40, 'happy': 100, 'neutral': 65, 'sad': 50, 'surprise': 55}\n",
    "    \n",
    "    return emotion_scores.get(emotion, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edcf6d0e-f031-488f-8d87-10936fcef61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames extracted:\n",
      "['frame_0.jpg', 'frame_1040.jpg', 'frame_1120.jpg', 'frame_1200.jpg', 'frame_1280.jpg', 'frame_1360.jpg', 'frame_1440.jpg', 'frame_160.jpg', 'frame_240.jpg', 'frame_320.jpg', 'frame_400.jpg', 'frame_480.jpg', 'frame_560.jpg', 'frame_640.jpg', 'frame_720.jpg', 'frame_80.jpg', 'frame_800.jpg', 'frame_880.jpg', 'frame_960.jpg']\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Emotion Scores:\n",
      "[65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65]\n",
      "Average Emotion Score: 65.0\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained emotion detection model\n",
    "model_path = r\"D:\\AI_interview\\facerec\\emotion_detector_final_model.h5\"\n",
    "emotion_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Define the emotion classes\n",
    "emotion_classes = ['angry', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "# Specify video path and output directory\n",
    "video_path = r\"D:\\AI_interview\\recorded\\answer_4.avi\"\n",
    "output_path = r\"D:\\AI_interview\\extracted\"\n",
    "\n",
    "# Extract frames from the video\n",
    "extract_frames(video_path, output_path)\n",
    "\n",
    "# Check the content of the output directory\n",
    "print(\"Frames extracted:\")\n",
    "print(os.listdir(output_path))\n",
    "\n",
    "# Predict emotions for each frame and calculate average emotion score\n",
    "emotion_scores = []\n",
    "\n",
    "for frame_file in os.listdir(output_path):\n",
    "    frame_path = os.path.join(output_path, frame_file)\n",
    "    predicted_emotion = predict_emotion(frame_path)\n",
    "    emotion_score = calculate_emotion_score(predicted_emotion)\n",
    "    emotion_scores.append(emotion_score)\n",
    "\n",
    "# Check the content of emotion_scores\n",
    "print(\"Emotion Scores:\")\n",
    "print(emotion_scores)\n",
    "\n",
    "# Check if emotion_scores is empty\n",
    "if not emotion_scores:\n",
    "    print(\"No frames were processed. Please check the frame extraction process.\")\n",
    "else:\n",
    "    # Calculate average emotion score\n",
    "    average_emotion_score = np.nanmean(emotion_scores)\n",
    "    print(\"Average Emotion Score:\", average_emotion_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
