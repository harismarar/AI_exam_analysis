{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84cbdea9-4eb3-444f-828b-84fb8a34c72e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msounddevice\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from scipy.io.wavfile import write, read\n",
    "import speech_recognition as sr\n",
    "import language_tool_python\n",
    "import spacy\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import librosa\n",
    "\n",
    "\n",
    "\n",
    "class VideoRecorderApp:\n",
    "    def __init__(self, master, question):\n",
    "        self.master = master\n",
    "        self.question = question\n",
    "        self.master.title(\"Video Recorder\")\n",
    "        self.master.geometry(\"800x600\")\n",
    "\n",
    "        self.label = tk.Label(self.master, text=f\"Question: {self.question}\")\n",
    "        self.label.pack()\n",
    "\n",
    "        self.start_button = tk.Button(self.master, text=\"Start Recording\", command=self.start_recording)\n",
    "        self.start_button.pack()\n",
    "\n",
    "        self.stop_button = tk.Button(self.master, text=\"Stop Recording\", command=self.stop_recording, state=tk.DISABLED)\n",
    "        self.stop_button.pack()\n",
    "\n",
    "        self.cap = None\n",
    "        self.out = None\n",
    "        self.audio_data = []\n",
    "\n",
    "    def start_recording(self):\n",
    "        self.start_button.config(state=tk.DISABLED)\n",
    "        self.stop_button.config(state=tk.NORMAL)\n",
    "\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        self.out = cv2.VideoWriter('D:\\\\AI_interview\\\\recorded\\\\answer.avi', fourcc, 20.0, (640, 480))\n",
    "        \n",
    "\n",
    "        def callback(indata, frames, time, status):\n",
    "            if status:\n",
    "                print(status, flush=True)\n",
    "            self.audio_data.append(indata.copy())\n",
    "\n",
    "        # Flag to check if 'e' key is pressed\n",
    "        stop_recording = False\n",
    "\n",
    "        # Start recording audio using the callback function\n",
    "        with sd.InputStream(callback=callback):\n",
    "            while True:\n",
    "                ret, frame = self.cap.read()\n",
    "                cv2.imshow('Recording', frame)\n",
    "                self.out.write(frame)\n",
    "\n",
    "                # Check if 'e' key is pressed\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('e'):\n",
    "                    stop_recording = True\n",
    "                    break\n",
    "\n",
    "        # Stop audio recording when 'e' key is pressed\n",
    "        sd.stop()\n",
    "\n",
    "        # Check if 'e' key was pressed during video recording\n",
    "        if stop_recording:\n",
    "\n",
    "            # Release video resources\n",
    "            self.cap.release()\n",
    "            self.out.release()\n",
    "\n",
    "            audio_array = np.concatenate(self.audio_data, axis=0)\n",
    "            audio_path = os.path.join(\"D:\\\\AI_interview\\\\recorded\", \"answer.wav\")\n",
    "            print(\"Saving audio file to:\", audio_path)  # Print the path\n",
    "            write(audio_path, 44100, audio_array.astype(np.int16))\n",
    "\n",
    "\n",
    "\n",
    "            # Close OpenCV windows\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "        else:\n",
    "            # If 'e' key wasn't pressed, release video resources and show a message\n",
    "            self.cap.release()\n",
    "            self.out.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            print(\"Recording stopped without saving audio.\")\n",
    "\n",
    "    def stop_recording(self):\n",
    "        self.start_button.config(state=tk.NORMAL)\n",
    "        self.stop_button.config(state=tk.DISABLED)\n",
    "\n",
    "        # Release video resources\n",
    "        self.cap.release()\n",
    "        self.out.release()\n",
    "\n",
    "        # Concatenate audio data and save as WAV file to specified path\n",
    "        audio_array = np.concatenate(self.audio_data, axis=0)\n",
    "        # Normalize the audio data before saving\n",
    "        normalized_audio = (audio_array / np.max(np.abs(audio_array)) * 32767).astype(np.int16)\n",
    "\n",
    "        # Concatenate audio data and save as WAV file to specified path\n",
    "        audio_path = os.path.join(\"D:\\\\AI_interview\\\\recorded\", \"answer.wav\")\n",
    "\n",
    "        print(\"Saving audio file to:\", audio_path)  # Print the path\n",
    "        write(audio_path, 44100, normalized_audio)\n",
    "\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "video_path = 'D:\\\\AI_interview\\\\recorded\\\\answer.avi'\n",
    "audio_path = 'D:\\\\AI_interview\\\\recorded\\\\answer.wav'\n",
    "\n",
    "\n",
    "\n",
    "def ask_question():\n",
    "    \n",
    "    print(\"Choose a question number between 1 and 6:\")\n",
    "    question_number = int(input())\n",
    "    questions = [\n",
    "        \"What is supervised learning and give some examples?\",\n",
    "        \"What is unsupervised learning and give some examples?\",\n",
    "        \"What is the definition of reinforcement learning?\",\n",
    "        \"What is overfitting and how can we avoid or solve it?\",\n",
    "        \"What is cross-validation and give an example?\",\n",
    "        \"What is bias-variance tradeoff?\"\n",
    "    ]\n",
    "\n",
    "    question = questions[question_number - 1]\n",
    "\n",
    "    root = tk.Tk()\n",
    "    app = VideoRecorderApp(root, question)\n",
    "    root.mainloop()\n",
    "\n",
    "    \n",
    "\n",
    "    def extract_frames(video_path, output_path):\n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # Get video properties\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = total_frames / fps\n",
    "\n",
    "        # Calculate the frame interval to get n/2 frames\n",
    "        frame_interval = int((total_frames * 4) /  duration)\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "\n",
    "        # Loop through the video and extract frames\n",
    "        current_frame = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "        \n",
    "            if not ret:\n",
    "                break  # Break the loop if no more frames are available\n",
    "\n",
    "            # Save the frame if it's at the interval\n",
    "            if current_frame % frame_interval == 0:\n",
    "                frame_filename = f\"{output_path}/frame_{current_frame}.jpg\"\n",
    "                cv2.imwrite(frame_filename, frame)\n",
    "\n",
    "            current_frame += 1\n",
    "\n",
    "        # Release the video capture object\n",
    "        cap.release()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict_emotion(frame_path):\n",
    "        img = cv2.imread(frame_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert to grayscale if needed\n",
    "        img = cv2.resize(img, (48, 48))  # Resize to match the input size of the model\n",
    "        img = img / 255.0  # Normalize pixel values\n",
    "    \n",
    "        img = img.reshape((1,) + img.shape + (1,))\n",
    "    \n",
    "        # Make prediction\n",
    "        emotion_probs = emotion_model.predict(img)\n",
    "    \n",
    "        # Get the emotion with the highest probability\n",
    "        predicted_emotion_video = emotion_classes_video[np.argmax(emotion_probs)]\n",
    "    \n",
    "        return predicted_emotion_video\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_emotion_score(emotion):\n",
    "        emotion_scores_video = {'angry': 40, 'fear': 40, 'happy': 100, 'neutral': 65, 'sad': 50, 'surprise': 55}\n",
    "    \n",
    "        return emotion_scores_video.get(emotion, 0)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "        # Load the pre-trained emotion detection model\n",
    "    video_model_path = r\"D:\\AI_interview\\facerec\\emotion_detector_final_model.h5\"\n",
    "    emotion_model = tf.keras.models.load_model(video_model_path)\n",
    "\n",
    "    # Define the emotion classes\n",
    "    emotion_classes_video = ['angry', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "    # Specify video path and output directory\n",
    "    output_path = r\"D:\\AI_interview\\extracted\"\n",
    "\n",
    "    # Extract frames from the video\n",
    "    extract_frames(video_path, output_path)\n",
    "\n",
    "    # Check the content of the output directory\n",
    "    print(\"Frames extracted:\")\n",
    "    print(os.listdir(output_path))\n",
    "\n",
    "    # Predict emotions for each frame and calculate average emotion score\n",
    "    emotions_scores_video = []\n",
    "\n",
    "    for frame_file in os.listdir(output_path):\n",
    "        frame_path = os.path.join(output_path, frame_file)\n",
    "        predicted_emotion_video = predict_emotion(frame_path)\n",
    "        video_emotion_score = calculate_emotion_score(predicted_emotion_video)\n",
    "        emotions_scores_video.append(video_emotion_score)\n",
    "\n",
    "    # Check the content of emotion_scores\n",
    "    print(\"Emotion Scores_video:\")\n",
    "    print(emotions_scores_video)\n",
    "\n",
    "    # Check if emotion_scores is empty\n",
    "    if not emotions_scores_video:\n",
    "        print(\"No frames were processed. Please check the frame extraction process.\")\n",
    "    else:\n",
    "        # Calculate average emotion score\n",
    "        average_emotion_score = np.nanmean(emotions_scores_video)\n",
    "        print(\"Average Emotion Score:\", int(average_emotion_score))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Load the trained model\n",
    "    audio_model_path = \"D:/AI_interview/audio_confidence/AI_audio2.h5\"\n",
    "    audio_model = tf.keras.models.load_model(audio_model_path)\n",
    "\n",
    "    # Define emotion classes\n",
    "    emotion_classes_audio = [\"happy\", \"angry\", \"sad\", \"neutral\", \"calm\", \"fear\", \"disgust\", \"surprise\"]\n",
    "\n",
    "\n",
    "    # Load the audio file and extract features\n",
    "    def extract_features(audio_path):\n",
    "        audio, sr = librosa.load(audio_path, sr=None)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "\n",
    "        # Pad or truncate the features to match the expected input shape (162 frames)\n",
    "        if mfccs.shape[1] < 162:\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, 162 - mfccs.shape[1])))\n",
    "        else:\n",
    "            mfccs = mfccs[:, :162]\n",
    "    \n",
    "        return mfccs\n",
    "\n",
    "    # Preprocess the audio for model prediction\n",
    "    input_data = extract_features(audio_path)\n",
    "    input_data = np.expand_dims(input_data, axis=-1)  # Add channel dimension\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_audio = audio_model.predict(input_data)\n",
    "\n",
    "    # Map predictions to emotion classes\n",
    "    predicted_class_index = np.argmax(predictions_audio)\n",
    "    predicted_emotion_audio = emotion_classes_audio[predicted_class_index]\n",
    "\n",
    "    # Score the prediction\n",
    "    emotion_score_audio = 0\n",
    "    confidence_score = 0\n",
    "\n",
    "    if predicted_emotion_audio in [\"happy\", \"calm\", \"neutral\"]:\n",
    "        emotion_score_audio = 100\n",
    "        confidence_score = 100\n",
    "    elif predicted_emotion_audio in [\"sad\", \"fear\", \"surprise\"]:\n",
    "        emotion_score_audio = 60\n",
    "        confidence_score = 50\n",
    "    else:  # [\"disgust\", \"angry\"]\n",
    "        emotion_score_audio = 40\n",
    "        confidence_score = 40\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Predicted Emotion: {predicted_emotion_audio}\")\n",
    "    print(f\"Emotion Score: {emotion_score_audio}\")\n",
    "    print(f\"Confidence Score: {confidence_score}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    supervised_learning = \"Supervised learning is a type of machine learning where the algorithm is trained on a labeled dataset, meaning that the input data is paired with corresponding output labels. The goal of supervised learning is to learn a mapping function from the input variables to the output variables, based on the labeled examples provided during training. In other words, the algorithm learns to make predictions or decisions by generalizing patterns from the labeled training data.Examples of supervised learning include image classification, spam email detection, handwriting recognition, speech recognition, predicting stock prices, language translation, medical diagnosis, customer churn prediction, autonomous vehicles, and credit scoring.\"\n",
    "    unsupervised_learning = \"Unsupervised learning is a machine learning paradigm where the algorithm is trained on unlabeled data, and the objective is to discover inherent patterns or structures within the data without explicit guidance. In unsupervised learning, the algorithm explores the data's inherent structure, often through clustering or dimensionality reduction techniques. Examples of unsupervised learning include clustering similar documents in a large corpus, anomaly detection to identify unusual patterns, dimensionality reduction for feature extraction, topic modeling to discover themes in text data, and generative modeling for creating new data instances based on the learned patterns\"\n",
    "    reinforcement_learning = \"Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions. The goal is for the agent to learn a policy that maximizes the cumulative reward over time.\"\n",
    "    overfitting = \"Overfitting occurs when a machine learning model learns the training data too well, including its noise and outliers, leading to poor generalization on new, unseen data. It often results from models being too complex relative to the amount of training data. To address overfitting, techniques such as regularization, reducing model complexity, increasing the size of the training dataset, and using cross-validation to evaluate model performance on different subsets of data can be employed\"\n",
    "    cross_validation = \"Cross-validation is a technique used to assess a model's performance by partitioning the dataset into multiple subsets. The model is trained on some subsets and tested on others, allowing for a more robust evaluation. Common methods include k-fold cross-validation, where the data is divided into k subsets, and the model is trained and tested k times, each time using a different subset for testing.\"\n",
    "    bias_variance_tradeoff = \"The bias-variance tradeoff is a fundamental concept in machine learning that deals with finding the right level of model complexity. High bias (underfitting) occurs when a model is too simple and cannot capture the underlying patterns in the data. High variance (overfitting) occurs when a model is too complex and fits the training data too closely, failing to generalize well to new data. Balancing bias and variance is crucial for building models that perform well on diverse datasets. Regularization techniques and model selection are commonly used to manage the bias-variance tradeoff\"\n",
    "\n",
    "\n",
    "    r = sr.Recognizer()\n",
    "\n",
    "\n",
    "    def speech_to_text(audio_file):\n",
    "      \"\"\"\n",
    "      Opens and listens to an audio file and translates it to text\n",
    "      Args: audio file\n",
    "      Returns: text of transcribed audio file\n",
    "      \"\"\"\n",
    "      with sr.AudioFile(audio_file) as source:\n",
    "        audio_text = r.listen(source)\n",
    "        try:\n",
    "            text = r.recognize_google(audio_text)\n",
    "            print('Converting audio transcripts into text ...')\n",
    "            # print(text)\n",
    "        except:\n",
    "             print('Sorry.. run again...')\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def extract_keywords(text):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc = nlp(text)\n",
    "        keywords = [token.text for token in doc if token.is_alpha]\n",
    "        return keywords\n",
    "\n",
    "\n",
    "\n",
    "    def check_semantic(text):\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Example: Check for specific semantic elements like named entities\n",
    "        entities = [ent.text for ent in doc.ents]\n",
    "\n",
    "        # You can also use dependency parsing to analyze sentence structure\n",
    "        # For example, check if certain words are connected in a specific way\n",
    "        root_verb = [token.text for token in doc if token.dep_ == 'ROOT' and token.pos_ == 'VERB']\n",
    "\n",
    "        # Example custom scoring function: Assign a score based on the number of entities\n",
    "        # and the presence of a root verb\n",
    "        semantic_score = calculate_semantic_score(entities, root_verb)\n",
    "\n",
    "        return semantic_score\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_semantic_score(entities, root_verb):\n",
    "        # Example: Assign a higher score if entities and a root verb are present\n",
    "        score = 0\n",
    "        if entities:\n",
    "            score += 0.5\n",
    "        if root_verb:\n",
    "            score += 0.5\n",
    "        return score\n",
    "    \n",
    "\n",
    "\n",
    "    def check_syntax(text):\n",
    "        tool = language_tool_python.LanguageTool('en-US')\n",
    "        matches = tool.check(text)\n",
    "\n",
    "        # Count the number of matches\n",
    "        num_matches = len(matches)\n",
    "\n",
    "        # Assign a score based on the number of matches\n",
    "        # You can customize this scoring logic based on your requirements\n",
    "        score = (1 - (num_matches*0.035))  # For example, inverse of the number of matches\n",
    "\n",
    "        return score, matches\n",
    "\n",
    "\n",
    "\n",
    "    def score_answer(chosen_number, keywords):\n",
    "        correct_answers = [\n",
    "            \"Supervised learning is a type of machine learning where the algorithm is trained on a labeled dataset, meaning that the input data is paired with corresponding output labels. The goal of supervised learning is to learn a mapping function from the input variables to the output variables, based on the labeled examples provided during training. In other words, the algorithm learns to make predictions or decisions by generalizing patterns from the labeled training data.Examples of supervised learning include image classification, spam email detection, handwriting recognition, speech recognition, predicting stock prices, language translation, medical diagnosis, customer churn prediction, autonomous vehicles, and credit scoring.\",\n",
    "             \"Unsupervised learning is a machine learning paradigm where the algorithm is trained on unlabeled data, and the objective is to discover inherent patterns or structures within the data without explicit guidance. In unsupervised learning, the algorithm explores the data's inherent structure, often through clustering or dimensionality reduction techniques. Examples of unsupervised learning include clustering similar documents in a large corpus, anomaly detection to identify unusual patterns, dimensionality reduction for feature extraction, topic modeling to discover themes in text data, and generative modeling for creating new data instances based on the learned patterns\",\n",
    "            \"Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions. The goal is for the agent to learn a policy that maximizes the cumulative reward over time.\",\n",
    "            \"Overfitting occurs when a machine learning model learns the training data too well, including its noise and outliers, leading to poor generalization on new, unseen data. It often results from models being too complex relative to the amount of training data. To address overfitting, techniques such as regularization, reducing model complexity, increasing the size of the training dataset, and using cross-validation to evaluate model performance on different subsets of data can be employed\",\n",
    "            \"Cross-validation is a technique used to assess a model's performance by partitioning the dataset into multiple subsets. The model is trained on some subsets and tested on others, allowing for a more robust evaluation. Common methods include k-fold cross-validation, where the data is divided into k subsets, and the model is trained and tested k times, each time using a different subset for testing.\",\n",
    "            \"The bias-variance tradeoff is a fundamental concept in machine learning that deals with finding the right level of model complexity. High bias (underfitting) occurs when a model is too simple and cannot capture the underlying patterns in the data. High variance (overfitting) occurs when a model is too complex and fits the training data too closely, failing to generalize well to new data. Balancing bias and variance is crucial for building models that perform well on diverse datasets. Regularization techniques and model selection are commonly used to manage the bias-variance tradeoff\"\n",
    "        ]\n",
    "\n",
    "        correct_answer = correct_answers[chosen_number - 1]  # Adjust for 0-based index\n",
    "\n",
    "        keyword_match = set(keywords) & set(correct_answer.split())\n",
    "        keyword_score = len(keyword_match) / len(correct_answer.split()) * 100\n",
    "        return keyword_score\n",
    "\n",
    "\n",
    "    def calculate_scores(semantic, syntax, answer):\n",
    "        total_score = 0.1 * semantic + 0.1 * syntax + 0.8 * answer\n",
    "        return total_score\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_student_answer(audio_path, chosen_number):\n",
    "        # Assuming correct_answer_keywords is a list of keywords related to the correct answer\n",
    "        correct_answer_keywords = get_correct_answer_keywords(chosen_number)\n",
    "        # Rest of the code remains the same\n",
    "        text = speech_to_text(audio_path)\n",
    "        keywords = extract_keywords(text)\n",
    "        syntax_matches = check_syntax(text)\n",
    "        semantic_score = check_semantic(text)\n",
    "        answer_score = score_answer(chosen_number, keywords)  # Fix here\n",
    "        syntax_score = 100 - (len(syntax_matches) * 3.5)  # Adjust as needed\n",
    "        final_score = calculate_scores(semantic_score, syntax_score, answer_score)\n",
    "\n",
    "        return final_score\n",
    "\n",
    "    def get_correct_answer_keywords(chosen_number):\n",
    "        # Assuming chosen_number is between 1 and 6\n",
    "        chosen_index = chosen_number - 1\n",
    "        correct_answers = [\n",
    "            \"Supervised learning is a type of machine learning where the algorithm is trained on a labeled dataset, meaning that the input data is paired with corresponding output labels. The goal of supervised learning is to learn a mapping function from the input variables to the output variables, based on the labeled examples provided during training. In other words, the algorithm learns to make predictions or decisions by generalizing patterns from the labeled training data.Examples of supervised learning include image classification, spam email detection, handwriting recognition, speech recognition, predicting stock prices, language translation, medical diagnosis, customer churn prediction, autonomous vehicles, and credit scoring.\",\n",
    "            \"Unsupervised learning is a machine learning paradigm where the algorithm is trained on unlabeled data, and the objective is to discover inherent patterns or structures within the data without explicit guidance. In unsupervised learning, the algorithm explores the data's inherent structure, often through clustering or dimensionality reduction techniques. Examples of unsupervised learning include clustering similar documents in a large corpus, anomaly detection to identify unusual patterns, dimensionality reduction for feature extraction, topic modeling to discover themes in text data, and generative modeling for creating new data instances based on the learned patterns\",\n",
    "            \"Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions. The goal is for the agent to learn a policy that maximizes the cumulative reward over time.\",\n",
    "            \"Overfitting occurs when a machine learning model learns the training data too well, including its noise and outliers, leading to poor generalization on new, unseen data. It often results from models being too complex relative to the amount of training data. To address overfitting, techniques such as regularization, reducing model complexity, increasing the size of the training dataset, and using cross-validation to evaluate model performance on different subsets of data can be employed\",\n",
    "            \"Cross-validation is a technique used to assess a model's performance by partitioning the dataset into multiple subsets. The model is trained on some subsets and tested on others, allowing for a more robust evaluation. Common methods include k-fold cross-validation, where the data is divided into k subsets, and the model is trained and tested k times, each time using a different subset for testing.\",\n",
    "            \"The bias-variance tradeoff is a fundamental concept in machine learning that deals with finding the right level of model complexity. High bias (underfitting) occurs when a model is too simple and cannot capture the underlying patterns in the data. High variance (overfitting) occurs when a model is too complex and fits the training data too closely, failing to generalize well to new data. Balancing bias and variance is crucial for building models that perform well on diverse datasets. Regularization techniques and model selection are commonly used to manage the bias-variance tradeoff\"\n",
    "        ]\n",
    "\n",
    "        return extract_keywords(correct_answers[chosen_index])\n",
    "\n",
    "    # Example usage\n",
    "    chosen_number = question_number  # Assuming the student picked 1\n",
    "    paths = audio_path\n",
    "    score = evaluate_student_answer(paths, chosen_number)\n",
    "    print(f\"Final Score: {score}\")\n",
    "\n",
    "    def final_mark(a,b,c,d):\n",
    "        final_marks = (0.2 * a) + (0.2 * b) + (0.1 * c) + (0.5 * d)\n",
    "        return final_marks\n",
    "\n",
    "    total = final_mark(average_emotion_score,emotion_score_audio,confidence_score,score)\n",
    "    print(\"final marks of the student is :\",total)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ask_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983d5e3-dc71-441b-b336-bed81268336e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
